"""Step 1: ê°„ë‹¨í•œ ì¸í„°ë™í‹°ë¸Œ ì±— (ê¸°ì–µ ì—†ìŒ - ë§¤ë²ˆ ë…ë¦½ì )

1ë‹¨ê³„ ìˆ˜ì¤€: LLM Providerë§Œ êµ¬í˜„ëœ ìƒíƒœ
- ê¸°ì–µ ê¸°ëŠ¥ ì—†ìŒ
- ë§¤ë²ˆ ìƒˆë¡œìš´ ë©”ì‹œì§€ë§Œ ì „ì†¡
"""

from src.llm.ollama_provider import OllamaProvider


def main():
    """ì¸í„°ë™í‹°ë¸Œ ì±— ë£¨í”„"""
    print("=" * 60)
    print("ë¡œì»¬ LLM ì±— (ê¸°ì–µ ì—†ìŒ - ë§¤ë²ˆ ë…ë¦½ì )")
    print("ì¢…ë£Œí•˜ë ¤ë©´ 'quit' ë˜ëŠ” 'exit' ì…ë ¥")
    print("=" * 60)
    
    # Provider ìƒì„±
    provider = OllamaProvider(model="llama3")
    
    while True:
        # ì‚¬ìš©ì ì…ë ¥
        user_input = input("\n[ë‹¹ì‹ ]: ").strip()
        
        # ì¢…ë£Œ ëª…ë ¹
        if user_input.lower() in ['quit', 'exit', 'ì¢…ë£Œ', 'q']:
            print("\nğŸ‘‹ ì•ˆë…•íˆê°€ì„¸ìš”!")
            break
        
        if not user_input:
            continue
        
        try:
            # ë©”ì‹œì§€ êµ¬ì„± (ë§¤ë²ˆ ë…ë¦½ì  - ê¸°ì–µ ì—†ìŒ)
            messages = [
                {"role": "user", "content": user_input}
            ]
            
            print("\n[LLM]: ", end="", flush=True)
            
            # LLM í˜¸ì¶œ
            response = provider.generate(messages, temperature=0.7)
            
            # ì‘ë‹µ ì¶œë ¥
            print(response)
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ ì•ˆë…•íˆê°€ì„¸ìš”!")
            break
        except Exception as e:
            print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            print("ğŸ’¡ Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”: ollama serve")


if __name__ == "__main__":
    main()

