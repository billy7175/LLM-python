"""Step 2: Context Assemblerë¥¼ ì‚¬ìš©í•œ ì¸í„°ë™í‹°ë¸Œ ì±—

step1_chat_with_memory.pyì™€ì˜ ì°¨ì´:
- step1: conversation_historyë¥¼ ê·¸ëŒ€ë¡œ LLMì— ì „ì†¡ (í† í° ì œí•œ ì—†ìŒ)
- step2: Context Assemblerê°€ í† í° ì œí•œ ë‚´ì—ì„œ ë©”ëª¨ë¦¬ë¥¼ ì„ íƒí•˜ì—¬ ì¡°ë¦½

Context Assemblerì˜ ì¥ì :
1. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìë™ ì¶”ê°€
2. í† í° ì œí•œ ê´€ë¦¬ (max_tokens ë‚´ì—ì„œë§Œ ë©”ëª¨ë¦¬ ì„ íƒ)
3. ë©”ì‹œì§€ ìˆœì„œ ê°•ì œ (system â†’ memories â†’ user)
"""

from src.llm.ollama_provider import OllamaProvider
from src.prompt.context_assembler import ContextAssembler


def main():
    """ì¸í„°ë™í‹°ë¸Œ ì±— ë£¨í”„ (Context Assembler ì‚¬ìš©)"""
    print("=" * 60)
    print("ë¡œì»¬ LLM ì±— (Context Assembler ì‚¬ìš© - í† í° ì œí•œ ê´€ë¦¬)")
    print("ì¢…ë£Œí•˜ë ¤ë©´ 'quit' ë˜ëŠ” 'exit' ì…ë ¥")
    print("=" * 60)
    
    # Provider ìƒì„±
    provider = OllamaProvider(model="llama3")
    
    # Context Assembler ìƒì„± (í† í° ì œí•œ ì„¤ì •)
    context_assembler = ContextAssembler(max_tokens=4096)
    
    # ëŒ€í™” ê¸°ë¡ ì €ì¥ (ë©”ëª¨ë¦¬ - í”„ë¡œê·¸ë¨ ì¢…ë£Œ ì‹œ ì‚¬ë¼ì§)
    conversation_history = []
    
    while True:
        # ì‚¬ìš©ì ì…ë ¥
        user_input = input("\n[ë‹¹ì‹ ]: ").strip()
        
        # ì¢…ë£Œ ëª…ë ¹
        if user_input.lower() in ['quit', 'exit', 'ì¢…ë£Œ', 'q']:
            print("\nğŸ‘‹ ì•ˆë…•íˆê°€ì„¸ìš”!")
            break
        
        if not user_input:
            continue
        
        try:
            print("\n[LLM]: ", end="", flush=True)
            
            # Context Assemblerë¡œ ë©”ì‹œì§€ ì¡°ë¦½ (í† í° ì œí•œ ê´€ë¦¬)
            messages = context_assembler.build_context(
                memories=conversation_history,
                user_message=user_input
            )
            
            # LLM í˜¸ì¶œ
            response = provider.generate(messages, temperature=0.7)
            
            # ì‚¬ìš©ì ë©”ì‹œì§€ì™€ LLM ì‘ë‹µì„ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
            conversation_history.append({
                "role": "user",
                "content": user_input
            })
            conversation_history.append({
                "role": "assistant",
                "content": response
            })
            
            # ì‘ë‹µ ì¶œë ¥
            print(response)
            
            # ë””ë²„ê·¸ ì •ë³´ (ì„ íƒì‚¬í•­)
            selected_count = len(messages) - 2  # systemê³¼ user ì œì™¸
            print(f"\n[ë””ë²„ê·¸] ì „ì²´ ë©”ëª¨ë¦¬: {len(conversation_history)}ê°œ, ì„ íƒëœ ë©”ëª¨ë¦¬: {selected_count}ê°œ")
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ ì•ˆë…•íˆê°€ì„¸ìš”!")
            break
        except Exception as e:
            print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            print("ğŸ’¡ Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”: ollama serve")


if __name__ == "__main__":
    main()

